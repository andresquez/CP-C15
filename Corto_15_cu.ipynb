{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Universidad del Valle de Guatemala** <br>\n",
        "**Computación Paralela** <br>\n",
        "**Corto 15** <br>\n",
        "**Andres Quezada 21085**"
      ],
      "metadata": {
        "id": "tU5S0jXR3dQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbcjH98C34zV",
        "outputId": "ebf51f00-9155-402b-f05e-44f1430f52d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduceSum.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "// Kernel CUDA para reducción paralela optimizada\n",
        "__global__ void reduceSum(int *input, int *output, int n) {\n",
        "    extern __shared__ int sharedData[];\n",
        "\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 2 + threadIdx.x;\n",
        "\n",
        "    // Cargar elementos en memoria compartida para utilizar la coalescencia de memoria\n",
        "    int sum = 0;\n",
        "    if (idx < n)\n",
        "        sum = input[idx];\n",
        "    if (idx + blockDim.x < n)\n",
        "        sum += input[idx + blockDim.x];\n",
        "    sharedData[tid] = sum;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Realizar la reducción en memoria compartida\n",
        "    for (unsigned int s = blockDim.x / 2; s > 32; s >>= 1) {\n",
        "        if (tid < s) {\n",
        "            sharedData[tid] += sharedData[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Desenrollar el último warp para evitar __syncthreads()\n",
        "    if (tid < 32) {\n",
        "        volatile int *vsmem = sharedData;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid + 8];\n",
        "        vsmem[tid] += vsmem[tid + 4];\n",
        "        vsmem[tid] += vsmem[tid + 2];\n",
        "        vsmem[tid] += vsmem[tid + 1];\n",
        "    }\n",
        "\n",
        "    // Escribir el resultado de este bloque en memoria global\n",
        "    if (tid == 0)\n",
        "        output[blockIdx.x] = sharedData[0];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    // Verificar si se proporcionó el argumento del exponente\n",
        "    if (argc != 2) {\n",
        "        fprintf(stderr, \"Usage: %s <exponent>\\n\", argv[0]);\n",
        "        fprintf(stderr, \"Example: %s 24 (for N = 1 << 24)\\n\", argv[0]);\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // Convertir el argumento a un entero\n",
        "    int exponent = atoi(argv[1]);\n",
        "    if (exponent < 0 || exponent > 30) {\n",
        "        fprintf(stderr, \"Exponent must be between 0 and 30.\\n\");\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    int N = 1 << exponent;\n",
        "    size_t size = N * sizeof(int);\n",
        "\n",
        "    // Asignar e inicializar memoria en el host\n",
        "    int *h_input = (int *)malloc(size);\n",
        "    if (h_input == NULL) {\n",
        "        fprintf(stderr, \"Failed to allocate host vectors!\\n\");\n",
        "        return -1;\n",
        "    }\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_input[i] = rand() % 100; // Valores aleatorios entre 0 y 99\n",
        "    }\n",
        "\n",
        "    // Asignar memoria en el dispositivo\n",
        "    int *d_input, *d_output;\n",
        "    cudaMalloc((void **)&d_input, size);\n",
        "\n",
        "    // Crear eventos CUDA para medir el tiempo\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Iniciar medición de tiempo desde la transferencia de datos CPU -> GPU\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    // Copiar datos desde el host al dispositivo\n",
        "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Determinar tamaños de bloque y grid\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock * 2 - 1) / (threadsPerBlock * 2);\n",
        "\n",
        "    // Asignar arreglo de salida en el dispositivo\n",
        "    int *h_partialSums = (int *)malloc(blocksPerGrid * sizeof(int));\n",
        "    cudaMalloc((void **)&d_output, blocksPerGrid * sizeof(int));\n",
        "\n",
        "    // Lanzar el kernel\n",
        "    reduceSum<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, N);\n",
        "\n",
        "    // Copiar las sumas parciales de vuelta al host\n",
        "    cudaMemcpy(h_partialSums, d_output, blocksPerGrid * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Detener la medición de tiempo después de la transferencia de datos GPU -> CPU\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float elapsedTime;\n",
        "    cudaEventElapsedTime(&elapsedTime, start, stop);\n",
        "\n",
        "    // Reducción final en el host\n",
        "    int gpu_sum = 0;\n",
        "    for (int i = 0; i < blocksPerGrid; i++) {\n",
        "        gpu_sum += h_partialSums[i];\n",
        "    }\n",
        "\n",
        "    // Validar el resultado realizando la suma en la CPU\n",
        "    // Medir el tiempo de la CPU\n",
        "    clock_t cpu_start = clock();\n",
        "    int cpu_sum = 0;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        cpu_sum += h_input[i];\n",
        "    }\n",
        "    clock_t cpu_end = clock();\n",
        "    float cpu_time = 1000.0 * (cpu_end - cpu_start) / CLOCKS_PER_SEC;\n",
        "\n",
        "    printf(\"Array Size (N): %d\\n\", N);\n",
        "    printf(\"CPU Sum: %d\\n\", cpu_sum);\n",
        "    printf(\"GPU Sum: %d\\n\", gpu_sum);\n",
        "    printf(\"Difference: %d\\n\", abs(cpu_sum - gpu_sum));\n",
        "    printf(\"GPU Time (including data transfer): %f ms\\n\", elapsedTime);\n",
        "    printf(\"CPU Time: %f ms\\n\", cpu_time);\n",
        "\n",
        "    // Liberar memoria\n",
        "    free(h_input);\n",
        "    free(h_partialSums);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUFjqJtJ5HPt",
        "outputId": "a681e74d-b8f6-4534-d990-b79dd17a4609"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduceSum.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o reduceSum reduceSum.cu\n"
      ],
      "metadata": {
        "id": "q4KYrE1Q5KGw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduceSum 24\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fqn-FLX5MO4",
        "outputId": "16fee792-fec5-4e55-98ae-78a073742cdc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array Size (N): 16777216\n",
            "CPU Sum: 830584179\n",
            "GPU Sum: 830584179\n",
            "Difference: 0\n",
            "GPU Time (including data transfer): 15.253568 ms\n",
            "CPU Time: 43.313999 ms\n"
          ]
        }
      ]
    }
  ]
}